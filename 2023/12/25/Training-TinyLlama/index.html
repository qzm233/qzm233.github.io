<!DOCTYPE html>
<html lang="en">
    
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <meta name="description" content="Training TinyLlama to clinic domain." />
    <meta name="hexo-theme-A4" content="v1.8.8" />
    <link rel="alternate icon" type="image/webp" href="/img/favicon.webp">
    <title>Hello,World. This is Zimo.</title>

    
        
<link rel="stylesheet" href="/css/highlight/style1.css">

        
<link rel="stylesheet" href="/css/reset.css">

        
<link rel="stylesheet" href="/css/markdown.css">

        
<link rel="stylesheet" href="/css/fonts.css">
 
         <!--Ê≥®ÊÑèÔºöÈ¶ñÈ°µÊó¢‰∏çÊòØpost‰πü‰∏çÊòØpage-->
        
        
        
<link rel="stylesheet" href="/css/ui.css">
 
        
<link rel="stylesheet" href="/css/style.css">


        
            <!--ËøîÂõûÈ°∂ÈÉ®css-->
            
<link rel="stylesheet" href="/css/returnToTop.css">

            
<link rel="stylesheet" href="/css/unicons.css">

        
        
            <!--ÁõÆÂΩï-->
            
<link rel="stylesheet" href="/css/toc.css">

        
    

    
        
<link rel="stylesheet" href="/css/returnToLastPage.css">

    
    
   
<link rel="stylesheet" href="/css/lightgallery.min.css">


<meta name="generator" content="Hexo 7.0.0"></head>
    
    
    <body>
        
            <div class="left-toc-container">
                <nav id="toc" class="bs-docs-sidebar"></nav>
            </div>
        
        <div class="paper">
            
            
            
            
                <div class="shadow-drop-2-bottom paper-main">
                    


<div class="header">
    <div class="header-container">
        <img style="
        width: 56px;
        height: auto;" alt="^-^" cache-control="max-age=86400" class="header-img" src="/img/favicon.webp" width="10%"></img>
        <div class="header-content">
            <a class="logo" href="/">Hello,World. This is Zimo.</a> 
            <span class="description"></span> 
        </div>
        
    </div>
    
   
    <ul class="nav">
        
            
                <li><a href="/">È¶ñÈ°µ</a></li>
            
        
            
                <li><a href="/list/">ÊñáÁ´†</a></li>
            
        
    </ul>
</div> 
        
                    
                    

                    
                    
                    
                    <!--ËØ¥ÊòéÊòØÊñáÁ´†postÈ°µÈù¢-->
                    
                        <div class="post-main">

    
        <div class="post-main-title">
            Training TinyLlama to clinic domain.
        </div>
      
    

    <div class="post-md">
        
            
                <ol class="post-toc"><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Introduction-of-LLMs"><span class="post-toc-text">Introduction of LLMs</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#The-Importance-of-LLMs-in-Specific-Domains"><span class="post-toc-text">The Importance of LLMs in Specific Domains</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Fine-Tuning-LLMs-for-Specific-Domains"><span class="post-toc-text">Fine-Tuning LLMs for Specific Domains</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#Potential-Directions-for-Applying-LLMs"><span class="post-toc-text">Potential Directions for Applying LLMs</span></a></li><li class="post-toc-item post-toc-level-2"><a class="post-toc-link" href="#My-trail-of-Finetuning-TinyLLaMA-1b-with-clinical-instruction-data-with-QLoRA-method"><span class="post-toc-text">My trail of Finetuning TinyLLaMA-1b with clinical instruction data with QLoRA method.</span></a></li></ol>
            
        
        <link rel="stylesheet" type="text/css" href="https://jsd.onmicrosoft.cn/npm/hexo-theme-a4@latest/source/css/lightgallery.min.css" /><div class=".article-gallery"><h2 id="Introduction-of-LLMs"><a href="#Introduction-of-LLMs" class="headerlink" title="Introduction of LLMs"></a>Introduction of LLMs</h2><p>LLMs, or Large Language Models, are advanced AI systems trained on vast amounts of text data. They leverage deep learning techniques, particularly the Transformer architecture, to learn patterns and structures in language. LLMs are capable of generating coherent text, understanding context, and performing complex language tasks such as translation, summarization, and question-answering.</p>
<h2 id="The-Importance-of-LLMs-in-Specific-Domains"><a href="#The-Importance-of-LLMs-in-Specific-Domains" class="headerlink" title="The Importance of LLMs in Specific Domains"></a>The Importance of LLMs in Specific Domains</h2><p>In specialized fields like clinical practice, law, and finance, the accuracy and expertise of information are paramount. The application of LLMs can enhance work efficiency and quality in these domains while minimizing human errors. For instance, in clinical settings, LLMs can assist doctors in quickly accessing medical records and literature to aid in diagnosis; in legal practice, they can help lawyers with case research and drafting legal documents; in finance, LLMs can provide market analysis and risk assessments.</p>
<h2 id="Fine-Tuning-LLMs-for-Specific-Domains"><a href="#Fine-Tuning-LLMs-for-Specific-Domains" class="headerlink" title="Fine-Tuning LLMs for Specific Domains"></a>Fine-Tuning LLMs for Specific Domains</h2><p>To make LLMs more effective in specific domains, they need to be fine-tuned. Fine-tuning involves training the model on a dataset specific to the target domain, adjusting the model‚Äôs parameters to adapt to the new context. The fine-tuning process typically includes the following steps:</p>
<ol>
<li><strong>Data Collection</strong>: Gather a large corpus of text data relevant to the target domain.</li>
<li><strong>Data Preprocessing</strong>: Clean and format the data to ensure it‚Äôs suitable for model training.</li>
<li><strong>Fine-Tuning Training</strong>: Train the model on the preprocessed dataset, adjusting parameters to adapt to the new domain.</li>
<li><strong>Evaluation and Iteration</strong>: Evaluate the model‚Äôs performance in the new domain and iterate as necessary for optimization.</li>
</ol>
<h2 id="Potential-Directions-for-Applying-LLMs"><a href="#Potential-Directions-for-Applying-LLMs" class="headerlink" title="Potential Directions for Applying LLMs"></a>Potential Directions for Applying LLMs</h2><ol>
<li><strong>Clinical Diagnostic Assistance</strong>: LLMs can analyze medical records to help doctors identify disease patterns and provide diagnostic suggestions.</li>
<li><strong>Legal Document Automation</strong>: Automatically generate contracts, legal documents, and legal opinions to reduce the workload of lawyers.</li>
<li><strong>Financial Risk Assessment</strong>: Analyze financial data to predict market trends and support investment decisions.</li>
<li><strong>Automated Customer Service</strong>: Provide natural language processing capabilities in customer service to improve response times and customer satisfaction.</li>
<li><strong>Regulatory Compliance</strong>: Assist in ensuring that businesses comply with regulatory requirements by analyzing and flagging potential non-compliance issues.</li>
<li><strong>Research and Development</strong>: In scientific research, LLMs can help identify relevant literature, summarize findings, and even suggest new research directions based on existing knowledge.</li>
<li><strong>Personalized Recommendations</strong>: In various industries, LLMs can tailor recommendations to individual needs, preferences, or circumstances, enhancing user experiences.</li>
</ol>
<h2 id="My-trail-of-Finetuning-TinyLLaMA-1b-with-clinical-instruction-data-with-QLoRA-method"><a href="#My-trail-of-Finetuning-TinyLLaMA-1b-with-clinical-instruction-data-with-QLoRA-method" class="headerlink" title="My trail of Finetuning TinyLLaMA-1b with clinical instruction data with QLoRA method."></a>My trail of Finetuning TinyLLaMA-1b with clinical instruction data with QLoRA method.</h2><ol>
<li><strong>Model</strong> <a target="_blank" rel="noopener" href="https://modelscope.cn/models/chaoscodes/TinyLlama-1.1B-Chat-v0.1">TinyLLaMA-1b</a></li>
<li><strong>Data</strong> <a target="_blank" rel="noopener" href="https://github.com/Kent0n-Li/ChatDoctor">clinical instruction data</a></li>
<li><strong>QLoRa Finetuning</strong> One can check the github code to find the method of fine-tuning with <a target="_blank" rel="noopener" href="https://github.com/artidoro/qlora">QLoRa</a>. Due to some constrains, only one epoch is conducted. QLoRa truely promote the training speed. There are some blogs that one can refer to.<a target="_blank" rel="noopener" href="https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/">Blog by Mathieu Busquet</a> and [blog by ]</li>
<li><strong>Validation on MedMCQA dataset</strong> <a target="_blank" rel="noopener" href="https://github.com/medmcqa/medmcqa">MedMCQA</a> is designed to address realworld medical entrance exam questions. After one epoch finetuning, LLM always talks foolishly. Because of finetuning dataset, the model is always act like a Robot but not an Examnee. One should add the proper prompt to get an answer of option and the accuracy is about 20%. One can refer to the <a target="_blank" rel="noopener" href="https://saankhya.medium.com/mistral-instruct-7b-finetuning-on-medmcqa-dataset-6ec2532b1ff1">blog by Saankhya Mondal</a>.</li>
</ol>
</div><script src="https://jsd.onmicrosoft.cn/npm/hexo-theme-a4@latest/source/js/lightgallery.min.js"></script><script>if (typeof lightGallery !== 'undefined') {
        var options = {
            selector: '.gallery-item'
        };
        lightGallery(document.getElementsByClassName('.article-gallery')[0], options);
        }</script>
    </div>

    <div class="post-meta">
        <i>
        
            <span>2023-12-25</span>
            
                <span>ËØ•ÁØáÊñáÁ´†Ë¢´ Zimo</span>
            
            
                <span>Êâì‰∏äÊ†áÁ≠æ:
                    
                    
                        <a href='/tags/LLM/'>
                            LLM
                        </a>
                    
                        <a href='/tags/clinic/'>
                            clinic
                        </a>
                    
                </span>
             
             
        
        </i>
    </div>
    <br>
    
    <!-- <div class="post-footer-pre-next">
        <span>‰∏ä‰∏ÄÁØáÔºö<a href=""></a></span>
        <span class="post-footer-pre-next-last-span-right">‰∏ä‰∏ÄÁØáÔºö<a href=""></a></span>
    </div> -->

    
        

     
</div>



                                      
                    
                    
                    <div class="footer">
    
        <span> 
            ¬© 1949-2024 China 

            
                

            
        </span>
    
</div>
<!--ËøôÊòØÊåá‰∏ÄÊù°Á∫øÂæÄ‰∏ãÁöÑÂÜÖÂÆπ-->
<div class="footer-last">
    
            <span>üåäÁúãËøáÂ§ßÊµ∑ÁöÑ‰∫∫‰∏ç‰ºöÂøòËÆ∞Êµ∑ÁöÑÂπøÈòîüåä</span>
            
                <span class="footer-last-span-right"><i>Êú¨Á´ôÁî±<a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/index.html">Hexo</a>È©±Âä®ÔΩú‰ΩøÁî®<a target="_blank" rel="noopener" href="https://github.com/HiNinoJay/hexo-theme-A4">Hexo-theme-A4</a>‰∏ªÈ¢ò</i></span>
            
    
</div>


    
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.0/jquery.min.js"></script>

    <!--ÁõÆÂΩï-->
    
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.7.2/jquery.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jqueryui/1.12.1/jquery-ui.min.js" type="text/javascript" ></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.tocify/1.9.0/javascripts/jquery.tocify.min.js" type="text/javascript" ></script>
        
<script src="/js/toc.js"></script>

    

    
<script src="/js/randomHeaderContent.js"></script>

    <!--ÂõûÂà∞È°∂ÈÉ®ÊåâÈíÆ-->
    
        
<script src="/js/returnToTop.js"></script>

    

    
        
<script src="/js/returnToLastPage.js"></script>

    





<script src="/js/lightgallery.min.js"></script>



                </div>
            
            
                <!-- ÂõûÂà∞È°∂ÈÉ®ÁöÑÊåâÈíÆ-->  
                <div class="progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
            
                <!-- ËøîÂõûÁöÑÊåâÈíÆ-->  
                <div class="return-to-last-progress-wrap shadow-drop-2-bottom">
                    <svg class="progress-circle svg-content" width="100%" height="100%" viewBox="-1 -1 102 102">
                        <path d="M50,1 a49,49 0 0,1 0,98 a49,49 0 0,1 0,-98"/>
                    </svg>
                </div>
            
    </body>


    <!--ÊöóÈªëÊ®°Âºè-->
    <script src="/js/darkmode-js.min.js"></script>
    <script>
        function addDarkmodeWidget() {
        const options = {
            bottom: '53px', // default: '32px'
            right: 'unset', // default: '32px'
            left: '42px', // default: 'unset'
            time: '0.3s', // default: '0.3s'
            mixColor: '#fff', // default: '#fff'
            backgroundColor: ' #e4e4e4 ',  // default: '#fff'
            buttonColorDark: '#100f2c',  // default: '#100f2c'
            buttonColorLight: '#fff', // default: '#fff'
            saveInCookies: true, // default: true,
            label: 'üåì', // default: ''
            autoMatchOsTheme: true // default: true
        }
    
        const darkmode = new Darkmode(options);
        darkmode.showWidget();
        
        }
        window.addEventListener('load', addDarkmodeWidget);
    </script>
  
</html>